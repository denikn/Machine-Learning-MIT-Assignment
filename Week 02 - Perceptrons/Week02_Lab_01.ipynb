{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Week02_Lab_01.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denikn/Machine-Learning-MIT-Assignment/blob/main/Week%2002%20-%20Perceptrons/Week02_Lab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef957ed4"
      },
      "source": [
        "# Evaluating learning methods"
      ],
      "id": "ef957ed4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db9d5516"
      },
      "source": [
        "In HW 2, we will implement a very simple learning algorithm that, when given a set of possible hyperplanes and a data set \\mathcal{D}_{\\it train}D \n",
        "train (consisting of a data array and label vector), returns the hyperplane that minimizes the number of errors on the training set. In the lecture, we have also considered the Perceptron algorithm (notes) and will implement it in homework this week. We will go on to study a number of other algorithms, all of which take in data as input and return a classification hypothesis as output.\n",
        "\n",
        "In this lab, we will explore how to evaluate learning methods. You will have a checkoff conversation with a staff member at the end of the lab. After that, you're welcome to leave or stay to work on homework.\n",
        "\n",
        "Note the following notation and definitions, used throughout this lab:"
      ],
      "id": "db9d5516"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d1f1913"
      },
      "source": [
        "    A generator \\mathcal{G}G is a function that takes as input nn, the number of samples desired, and returns an \\mathcal{(X,y)}(X,y) pair where \\mathcal{X}X is a \\mathcal{d}d by \\mathcal{n}n array of randomly sampled data points and \\mathcal{y}y is a 1 by \\mathcal{n}n array of their corresponding labels \\{+1,−1\\}{+1,−1}.\n",
        "\n",
        "    A training dataset \\mathcal{D}_{\\it train}D train is a set of labelled samples generated by \\mathcal{G}G, \\mathcal{X,y}X,y, where x^{i}xi represents the features of an object to be classified (vector of real and/or discrete values), and y^{i}yi represents the label of x^{i}xi\n",
        "\n",
        "    A binary classifier hh is a function that takes an example x \\in R^dx∈Rd as input and returns +1+1 or −1−1 as output."
      ],
      "id": "2d1f1913"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ceaf4d"
      },
      "source": [
        "## 1) Evaluating a classifier"
      ],
      "id": "41ceaf4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e1ae1de"
      },
      "source": [
        "Imagine that you have a generator \\mathcal{G}G that pulls from a finite dataset of millions of points.\n",
        "\n",
        "Let's assume that \\mathcal{D}_{\\it train}D train is one such output of the generator \\mathcal{G}G.\n",
        "\n",
        "Consider the situation in which you have run a machine learning algorithm on some training dataset \\mathcal{D}_{\\it train}D \n",
        "train, and it has returned to you a specific hh. Your job is to design (but not implement yet!) a procedure for evaluating hh's effectiveness as a classifier. (Want more on classifiers? Check the notes)\n",
        "\n",
        "Assume we have a score function that takes a classifier hh, dataset DD - a tuple of data and labels: (X,y)(X,y) - and returns the percentage of correctly classified examples as a decimal between 0 and 1. We'll package it as follows:"
      ],
      "id": "5e1ae1de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0422287"
      },
      "source": [
        "def eval_classifier(h, D):\n",
        "    test_X, test_y = D\n",
        "    return score(h, test_X, test_y)"
      ],
      "id": "d0422287",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34fd5395"
      },
      "source": [
        "A) Percy Eptron suggests reusing the training data to assess hh:"
      ],
      "id": "34fd5395"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc16abc0"
      },
      "source": [
        "# eval_classifier(h, D_train)"
      ],
      "id": "bc16abc0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df24f830"
      },
      "source": [
        "Explain why Percy's strategy might not be so good.\n",
        "\n",
        "Answer : Because if we have for example three positive points and three negative point and there's currently a point with the circle on it and run the perceptron algorithm, the result will be theta is equal to 0 and theta naught is equal to 0 too. So it's no plotting. So we know that the first point is always considered a mistake. No matter what the first point is, or any example, if theta=0 and theta naught=0, then this is hoing to be 0 and this is going to be 0, and this is hoing to be less than or equal to 0. Then we get that one wrong if we apply to another data set, hypothesis is wrong. But it's hard to predict exactly what's hoing to happen, and difficult to analyst."
      ],
      "id": "df24f830"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54dd1791"
      },
      "source": [
        "B) Now write down a better approach for evaluating hh, which may use hh, \\mathcal{G}G, and \\mathcal{D}_{\\it train}D \n",
        "train, and computes a score for hh. The syntax is not important, but do write something down. What does this score measure and what is the range of possible outputs?"
      ],
      "id": "54dd1791"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874cf42b"
      },
      "source": [
        "t = 1 # just make this variable exception for error\n",
        "i = 1 # t and i start from 1\n",
        "\n",
        "def eval_classifier(t, D):\n",
        "    test_X, test_y = D\n",
        "    for t in t:\n",
        "        for i in n:\n",
        "            if validation <= 0:\n",
        "                test_X = update_test_X\n",
        "                test_Y = update_test_Y\n",
        "    return score(h, test_X, test_y)"
      ],
      "id": "874cf42b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26777aac"
      },
      "source": [
        "Answer : the score measure that the result will be work for existing hypothesis. The range of possible outputs may be same or like the validation."
      ],
      "id": "26777aac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b811d39a"
      },
      "source": [
        "C) Explain why your method might be more desirable than Percy's. What problem does it fix?\n",
        "\n",
        "Answer: Because my method use the validation from data train iteration process. So the algorithm will memorize result from train iteration and make validation for data test prediction. "
      ],
      "id": "b811d39a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8850767a"
      },
      "source": [
        "D) How would your method from B score the classifier hh, if \\mathcal{D}_{\\it test}D \n",
        "test came from a different distribution than \\mathcal{G}G, but \\mathcal{D}_{\\it train}D \n",
        "train was unchanged?\n",
        "\n",
        "Answer : the method will be score the data with memory from for data train. Not 100% valid prediction, same like we cannot perfectly remember all of our homework or something that happened in our live, but in generalize, we can learn something to answer what will happen in the future by experience. The method also do the same thing, it not just remember, but it also learn. So if we apply with new data, the method will be work properly but not 100% perfect."
      ],
      "id": "8850767a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf67a6b7"
      },
      "source": [
        "# 2) Evaluating a learning algorithm"
      ],
      "id": "cf67a6b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499d09e5"
      },
      "source": [
        "A learning algorithm is a function LL that takes as input\n",
        "\n",
        "    data set \\mathcal{D}_{\\it train}D train as training data\n",
        "    \n",
        "and returns\n",
        "    \n",
        "       a classifier hh."
      ],
      "id": "499d09e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f3c4ab"
      },
      "source": [
        "A) Would running the learning algorithm LL on two different training datasets \\mathcal{D}_{\\it train_1}Dtrain1 and \\mathcal{D}_{\\it train_2}Dtrain2 produce the same classifier? In other words, would h_1h1= L(\\mathcal{D}_{\\it train_1})L(D \n",
        "train1) be the same classifier as h_2h2 = L(\\mathcal{D}_{\\it train_2})L(Dtrain2)? What if those training datasets were pulled from the same distribution?\n",
        "\n",
        "Answer : I think it will be different. Because every single dataset need to be processed with specific algorithm. Even the data set have same distribution, the result might be different if we just applied to same classifier.\n",
        "\n",
        "Now, consider a situation in which someone is trying to sell you a new learning algorithm, and you want to know how good it is. There is an interesting result that says that without any assumptions about your data, There is no learning algorithm that, for all data sources, is better than every other learning algorithm. So, you'll need to assess the learning algorithm's performance in the context of a particular data source.\n",
        "\n",
        "Check Yourself: What is the difference between a classifier and a learning algorithm? Understanding the distinction will help you when thinking about this question. (Stuck? Check the notes)\n",
        "\n",
        "Assume that you have a generator of labeled data, \\mathcal{G}G, which will be suitable for your application. The learning algorithm's performance on \\mathcal{G}G-generated data will be a good predictor of the learning algorithm's performance on data from your application. (You can review how to evaluate learning algorithms in the notes)"
      ],
      "id": "b8f3c4ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "722e8f4a"
      },
      "source": [
        "B) Linnea Separatorix wants to evaluate a learning algorithm, and suggests the following procedure:"
      ],
      "id": "722e8f4a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3da98e86"
      },
      "source": [
        "def eval_learning_alg(L, G, n):\n",
        "    # draw a set of n training examples (points and labels)\n",
        "    train_X, train_y = G(n)\n",
        "    # run L\n",
        "    h = L(train_X, train_y)\n",
        "    # evaluate using your classifier scoring procedure, on some new labeled data\n",
        "    test_data = G(n) # draw new set of test data\n",
        "    return eval_classifier(h, test_data)"
      ],
      "id": "3da98e86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3ba4ca"
      },
      "source": [
        "Check Yourself: What are GG and nn in the code above?\n",
        "\n",
        "Answer : G is theta or the projection of L on to that direction. n is parameter on L classifier problem.\n",
        "\n",
        "Explain why Linnea's strategy might not be so good.\n",
        "\n",
        "Answer : Because if we take point x and l take the dot product with theta, this give us a value. Normally, if we're thinking about the separator, if the sign of this is positive, we would predict plus 1. If the sign is negative, we would predict minus 1. If we take that prediction not the sign, just the stuff inside and we multiply it by the target value, we're making a correct prediction when this product is positive because either this is negative. "
      ],
      "id": "0e3ba4ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4229d541"
      },
      "source": [
        "C) Next, Linnea decides to generate one classifier hh but evaluate that classifier with multiple (10) test sets in her eval_learning_alg. More specifically, Linnea changed her code above into:"
      ],
      "id": "4229d541"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "028741ed"
      },
      "source": [
        "def eval_learning_alg(L, G, n):\n",
        "    # draw a set of n training examples (points and labels)\n",
        "    train_X, train_y = G(n)\n",
        "    # run L\n",
        "    h = L(train_X, train_y)\n",
        "    # evaluate using your classifier scoring procedure, on some new labeled data\n",
        "    score = 0\n",
        "    for i in range(10):\n",
        "        test_data = G(n) # draw new set of test data\n",
        "        score += eval_classifier(h, test_data)\n",
        "    return score/10"
      ],
      "id": "028741ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5efc2e8b"
      },
      "source": [
        "Is Linnea's strategy good now? Explain why or why not.\n",
        "\n",
        "Answer : Yes. Because it use cross-validation, so there are many potential sources of variability in the possible result of computing test error on a learned hypothesis  h.\n",
        "\n",
        "Check Yourself: How many classifiers is Linea generating and testing from the learning algorithm? \n",
        "Answer : 10 algorithm"
      ],
      "id": "5efc2e8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e529cdc"
      },
      "source": [
        "D) Now design a better procedure for evaluating LL. Write pseudocode for a procedure that takes LL, \\mathcal{G}G and nn and returns a score. Say what the output score measures and what the best and worst values are."
      ],
      "id": "8e529cdc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "156b2746"
      },
      "source": [
        "def better_eval_learning_alg(L, G, n):\n",
        "    # draw a set of n training examples (points and labels)\n",
        "    train_X, train_y = G(n)\n",
        "    # run L\n",
        "    h = L(train_X, train_y)\n",
        "    # evaluate using your classifier scoring procedure, on some new labeled data\n",
        "    score = 0\n",
        "    for i in range(10):\n",
        "        if validation <= 0:\n",
        "            test_data = G(n) # draw new set of test data\n",
        "            score += eval_classifier(h, test_data)\n",
        "    return score/10"
      ],
      "id": "156b2746",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7455e9c4"
      },
      "source": [
        "E) Explain why your method might be more desirable than Linnea's.\n",
        "\n",
        "Answer : Because in my method, I use validation to validate if the test data produce error value, so the method doesnt process it."
      ],
      "id": "7455e9c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be57bcd9"
      },
      "source": [
        "## 3) Evaluating a learning algorithm with a small amount of data"
      ],
      "id": "be57bcd9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a4902e2"
      },
      "source": [
        "In reality, it's almost never possible to have a generator of all the data you want; in fact, in some domains data is very expensive to collect, and so you are given a fixed, small set of samples. Now assume that you only have 100 labeled data points to use for training and testing/evaluation."
      ],
      "id": "7a4902e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05a461f7"
      },
      "source": [
        "A) In the last section, you thought about how to evaluate a learning algorithm. Now that you are given only 100 labeled data points in total, how would you evaluate a learning algorithm? Specifically, how would you implement better_eval_learning_alg from 2C) without \\mathcal{G}G but instead with your 100 labeled data? (You don't need to write out new pseudocode for better_eval_learning_alg but still think about how you would implement it.)"
      ],
      "id": "05a461f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7d7cfad"
      },
      "source": [
        "Answer: I will separate those data into train and test data. Then I use the train data for training the classifier, then I use test data to predict the score. I will use cross validation, so the score will more desirable."
      ],
      "id": "d7d7cfad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f4e4b6d"
      },
      "source": [
        ""
      ],
      "id": "9f4e4b6d",
      "execution_count": null,
      "outputs": []
    }
  ]
}